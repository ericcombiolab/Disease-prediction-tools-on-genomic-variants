{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All filenames could be changed if you have different ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate simulated data from an additive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the individuals in the fam file into three parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "fam =       #the path and the name of fam file\n",
    "n1 =        #the number of individuals to be put into the training set\n",
    "n2 =        #the number of individuals to be put into the validation set\n",
    "n3 =        #the number of individuals to be put into the test set\n",
    "\n",
    "fam = pd.read_csv(fam,sep=' ',header=None)\n",
    "set1 = list(fam.iloc[:,0].values)\n",
    "fam1 = random.sample(set1,n1)\n",
    "set2 = list(set(set1).difference(set(fam1)))\n",
    "fam2 = random.sample(set2,n2)\n",
    "fam3 = random.sample(list(set(set2).difference(set(fam2))),n3)\n",
    "fam1 = pd.DataFrame(fam1)\n",
    "fam2 = pd.DataFrame(fam2)\n",
    "fam3 = pd.DataFrame(fam3)\n",
    "fam1[1] = fam1.iloc[:,0].values\n",
    "fam2[1] = fam2.iloc[:,0].values\n",
    "fam3[1] = fam3.iloc[:,0].values\n",
    "fam1.to_csv('train.txt',sep=' ',header=False,index=False)\n",
    "fam2.to_csv('valid.txt',sep=' ',header=False,index=False)\n",
    "fam3.to_csv('test.txt',sep=' ',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split raw genotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('plink --bfile raw_data --keep train.txt --make-bed --out training_set')\n",
    "os.system('plink --bfile raw_data --keep valid.txt --make-bed --out validation_set')\n",
    "os.system('plink --bfile raw_data --keep test.txt --make-bed --out test_set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select a list of risk SNPs and assign their effect sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "bim = pd.read_csv('bim file',sep='\\t',header=None)\n",
    "\n",
    "# here we have an example of 10000 risk SNPs with 2500 small effect sizes, 5000 moderate effect sizes \n",
    "# and 2500 strong effect sizes\n",
    "n1 = 10000\n",
    "n2 = 2500\n",
    "n3 = 5000\n",
    "n4 = 2500\n",
    "snp = random.sample(list(bim.iloc[:,1].values),n1)\n",
    "eff1 = np.random.normal(0,0.1,n2)\n",
    "eff2 = np.random.normal(0,0.1**0.5,n3)\n",
    "eff3 = np.random.normal(0,1,n4)\n",
    "eff = np.concatenate((eff1,eff2,eff3))\n",
    "snp = pd.DataFrame(snp)\n",
    "snp[1] = list(eff)\n",
    "snp.to_csv('causal.snplist',header=False,index=False,sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply GCTA to generate the phenotypes for the individuals in the training set, validation set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_cases_train = \n",
    "num_controls_train = \n",
    "num_cases_valid = \n",
    "num_controls_valid =\n",
    "num_cases_test = \n",
    "num_controls_test =\n",
    "heritability = \n",
    "prevalence =\n",
    "num_replicates =\n",
    "os.system('gcta64 --bfile training_set --simu-cc {} {} --simu-causal-loci causal.snplist --simu-hsq {} --simu-k {} --simu-rep {} --out output'.format(num_cases_train,num_controls_train,heritability,prevalence,num_replicates))\n",
    "os.system('gcta64 --bfile validation_set --simu-cc {} {} --simu-causal-loci causal.snplist --simu-hsq {} --simu-k {} --simu-rep {} --out output'.format(num_cases_valid,num_controls_valid,heritability,prevalence,num_replicates))\n",
    "os.system('gcta64 --bfile test_set --simu-cc {} {} --simu-causal-loci causal.snplist --simu-hsq {} --simu-k {} --simu-rep {} --out output'.format(num_cases_test,num_controls_test,heritability,prevalence,num_replicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate the association statistics by PLINK on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('plink --bfile training_set --logistic --out output') # other filtering parameters could be added (e.g. --maf 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate simulated data from a nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Use HapGen2 to generate genotype data (you could skip this part if you already have the target genotype data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('hapgen2 -m .map_file -l .legend_file -h .haps_file -o output -n number_of_controls number_of_cases -Ne effective_population_size -dl physical_location  risk_allele heterozygote_disease_risk homozygote_disease_risk -no_haps_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare genotype data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use SIMER to generate quantitative phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('Rscript generate_phenotypes.R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert quantitative phenotypes into binary phenotypes and generate training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def to_binary_phenotype(path,outpath,popu,chro):\n",
    "    random.seed(10)\n",
    "    df_fam = pd.read_csv('{}{}_chr{}.fam'.format(path,popu,chro),sep=' ',header=None)\n",
    "    boundary = np.quantile(df_fam.iloc[:,5].values,0.8) # 0.8 here means that we set the disease prevalence to be 0.2\n",
    "    index1 = df_fam.index[df_fam.iloc[:,5]>=boundary]\n",
    "    index2 = df_fam.index[df_fam.iloc[:,5]<boundary]\n",
    "    if len(index1) != 0.2*len(df_fam.index):\n",
    "        sys.exit('The number of cases is not correct')\n",
    "    df_fam.loc[index1,5] = 2\n",
    "    df_fam.loc[index2,5] = 1\n",
    "    df_fam[5] = df_fam[5].astype(int)\n",
    "    df_fam.to_csv('{}{}_chr{}.fam'.format(path,popu,chro),sep=' ',header=False,index=False)\n",
    "    n1 = 10000 # number of cases/controls in the training set\n",
    "    n2 = 5000 # number of cases/controls in the validation set\n",
    "    n3 = 3000 # number of cases/controls in the testing set\n",
    "    total_case = list(df_fam.loc[index1,0].values)\n",
    "    total_control = list(df_fam.loc[index2,0].values)\n",
    "\n",
    "    training_case = random.sample(total_case,n1)\n",
    "    total_case_left = list(set(total_case)-set(training_case))\n",
    "    valid_case = random.sample(total_case_left,n2)\n",
    "    test_case = list(set(total_case_left)-set(valid_case))\n",
    "\n",
    "    training_control = random.sample(total_control,n1)\n",
    "    total_control_left1 = list(set(total_control)-set(training_control))\n",
    "    valid_control = random.sample(total_control_left1,n2)\n",
    "    total_control_left2 = list(set(total_control_left1)-set(valid_control))\n",
    "    test_control = random.sample(total_control_left2,n3)\n",
    "\n",
    "    training = training_case+training_control\n",
    "    valid = valid_case+valid_control\n",
    "    testing = test_case+test_control\n",
    "    training = sorted(training)\n",
    "    valid = sorted(valid)\n",
    "    testing = sorted(testing)\n",
    "\n",
    "    df_train = pd.DataFrame(training)\n",
    "    df_train[1] = training\n",
    "    df_valid = pd.DataFrame(valid)\n",
    "    df_valid[1] = valid\n",
    "    df_test = pd.DataFrame(testing)\n",
    "    df_test[1] = testing\n",
    "\n",
    "    df_train.to_csv('{}{}_chr{}_training_id.txt'.format(path,popu,chro),sep=' ',header=False,index=False)\n",
    "    df_valid.to_csv('{}{}_chr{}_valid_id.txt'.format(path,popu,chro),sep=' ',header=False,index=False)\n",
    "    df_test.to_csv('{}{}_chr{}_test_id.txt'.format(path,popu,chro),sep=' ',header=False,index=False)\n",
    "\n",
    "    os.system('plink --bfile {}{}_chr{} --maf 0.01 --make-bed --out {}{}_chr{}_filtered'.format(path,popu,chro,path,popu,chro))\n",
    "    os.system('plink --bfile {}{}_chr{}_filtered --keep {}{}_chr{}_training_id.txt --make-bed --out {}{}_chr{}_training'.format(path,popu,chro,path,popu,chro,outpath,popu,chro))\n",
    "    os.system('plink --bfile {}{}_chr{}_filtered --keep {}{}_chr{}_valid_id.txt --make-bed --out {}{}_chr{}_valid'.format(path,popu,chro,path,popu,chro,outpath,popu,chro))\n",
    "    os.system('plink --bfile {}{}_chr{}_filtered --keep {}{}_chr{}_test_id.txt --make-bed --out {}{}_chr{}_test'.format(path,popu,chro,path,popu,chro,outpath,popu,chro))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some methods could use summary statistics below, others have required formats as shown in their documentations.\n",
    "## LDpred2 could use the summary statistics generated by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for phe in diseases:\n",
    "    if phe == '1002' or phe == '1348' or phe == '1207':\n",
    "        logi = pd.read_csv('./{}/{}_train.assoc.logistic'.format(phe,phe),sep=' +')\n",
    "        set1 = list(range(0,len(logi.index),12)) # here 12 is related to the number of factors in the covariance file. The goal is to only include 'additive' one, \n",
    "        # and skip others considering only one factor\n",
    "        logi1 = logi.iloc[set1,:]\n",
    "        logi1.index=list(range(len(logi1.index)))\n",
    "        logi1.to_csv('./{}/{}_train_v2.assoc.logistic'.format(phe,phe),sep=' ',index=False)\n",
    "    else:\n",
    "        logi = pd.read_csv('./{}/{}_train.assoc.logistic'.format(phe,phe),sep=' +')\n",
    "        set1 = list(range(0,len(logi.index),13)) # we have one more factor 'sex' here, so it is 13.\n",
    "        logi1 = logi.iloc[set1,:]\n",
    "        logi1.index=list(range(len(logi1.index)))\n",
    "        logi1.to_csv('./{}/{}_train_v2.assoc.logistic'.format(phe,phe),sep=' ',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweedie, tlpSum, elastSum, lassosum, EB-PRS could use the summary statistics generated by the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for phe in diseases:\n",
    "    logi = pd.read_csv('./{}/{}_train_v2.assoc.logistic'.format(phe,phe),sep=' +')\n",
    "    frq = pd.read_csv('./{}/{}_train.frq'.format(phe,phe),sep=' +')\n",
    "    set1=[]\n",
    "    for j in range(len(frq.index)):\n",
    "        if frq.loc[j,'SNP'] not in logi.loc[:,'SNP'].values:\n",
    "            set1.append(j)\n",
    "    frq.drop(set1,axis=0,inplace=True)\n",
    "    frq.index = range(len(frq.index))\n",
    "    if all(logi.loc[:,'SNP'].values == frq.loc[:,'SNP'].values):\n",
    "            logi['A2'] = frq.loc[:,'A2'].values\n",
    "            logi['MAF'] = frq.loc[:,'MAF'].values\n",
    "            name=['CHR','SNP','BP','A1','A2','STAT','P','TEST','NMISS','OR','SE','L95','U95','MAF']\n",
    "            logi=logi[name]\n",
    "            logi.to_csv('./{}/{}_train_v3.assoc.logistic'.format(phe,phe),sep=' ',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate SNP list with p-values < 5E-3, < 5E-4, < 5E-5, < 5E-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate(index,hsq):\n",
    "    print(index)\n",
    "    valid=defaultdict(float)\n",
    "    test=defaultdict(float)\n",
    "    infile1=open('../train_rep{}_{}.assoc.logistic.adjusted'.format(index,hsq))\n",
    "    snplist1=open('./SNPselection/'+index+'_5E3_SNP_{}'.format(hsq),'w')\n",
    "    snplist2=open('./SNPselection/'+index+'_5E4_SNP_{}'.format(hsq),'w')\n",
    "    snplist3=open('./SNPselection/'+index+'_5E5_SNP_{}'.format(hsq),'w')\n",
    "    snplist4=open('./SNPselection/'+index+'_5E6_SNP_{}'.format(hsq),'w')\n",
    "\n",
    "    SNP1=0\n",
    "    SNP2=0\n",
    "    SNP3=0\n",
    "    SNP4=0\n",
    "    \n",
    "    linenum=0\n",
    "    for line in infile1:\n",
    "        if linenum>0:\n",
    "            A=line.strip('\\n').rsplit()\n",
    "            if float(A[3])<0.005:\n",
    "                snplist1.write(A[1]+'\\n')\n",
    "                SNP1+=1\n",
    "            if float(A[3])<0.0005:\n",
    "                snplist2.write(A[1]+'\\n')\n",
    "                SNP2+=1\n",
    "            if float(A[3])<0.00005:\n",
    "                snplist3.write(A[1]+'\\n')\n",
    "                SNP3+=1\n",
    "            if float(A[3])<0.000005:\n",
    "                snplist4.write(A[1]+'\\n')\n",
    "                SNP4+=1\n",
    "        linenum+=1\n",
    "    snpvalidate.write(str(index)+' '+str(SNP1)+' '+str(SNP2)+' '+str(SNP3)+' '+str(SNP4)+'\\n')\n",
    "    infile1.close()\n",
    "    snplist1.close()\n",
    "    snplist2.close()\n",
    "    snplist3.close()\n",
    "    snplist4.close()\n",
    "for hsq in [0.5]:\n",
    "    snpvalidate=open('./SNPselection/stat_{}'.format(hsq),'w')\n",
    "    for index in range(1,11):\n",
    "        #print(index)\n",
    "        generate(str(index),str(hsq))\n",
    "    snpvalidate.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract SNPs satisfying p-values < 5E-3, < 5E-4, < 5E-5, < 5E-6 and remove individuals with phenotypes of -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def generate(index,hsq):\n",
    "    os.system('plink --bfile ../train_merge_white_Britich_clean_rep'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E6_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E6_train_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../train_merge_white_Britich_clean_rep'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E5_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E5_train_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../train_merge_white_Britich_clean_rep'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E4_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E4_train_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../train_merge_white_Britich_clean_rep'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E3_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E3_train_extract_'+hsq+'\\n')\n",
    "    \n",
    "    os.system('plink --bfile ../vali_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E6_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E6_valid_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../vali_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E5_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E5_valid_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../vali_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E4_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E4_valid_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../vali_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E3_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E3_valid_extract_'+hsq+'\\n')\n",
    "    \n",
    "    os.system('plink --bfile ../test_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E6_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E6_test_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../test_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E5_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E5_test_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../test_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E4_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E4_test_extract_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ../test_merge_white_Britich_clean_'+index+'_'+hsq+' --prune --extract ./SNPselection/'+index+'_5E3_SNP_'+hsq+' --make-bed --out ./SNPextraction/'+index+'_5E3_test_extract_'+hsq+'\\n')\n",
    "    \n",
    "\n",
    "for hsq in [0.5]:\n",
    "    for index in range(1,11):\n",
    "        generate(str(index),str(hsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LD pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def generate(index,hsq):\n",
    "    print(index)\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_train_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E6_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_train_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E5_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_train_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E4_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_train_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/train_5E3_'+index+'_'+hsq+'\\n')\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_valid_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_valid_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_valid_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_valid_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/valid_'+index+'_'+hsq+'\\n')\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_test_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_test_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_test_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'_'+hsq+'\\n')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_test_extract_'+hsq+' --indep-pairwise 50 5 0.5 --out ./LDpruning/test_'+index+'_'+hsq+'\\n')\n",
    "\n",
    "\n",
    "for hsq in [0.5]:\n",
    "    for index in range(1,11):\n",
    "        generate(str(index),str(hsq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract SNPs that are low LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate(index,hsq):\n",
    "    print(index)\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_train_extract_'+hsq+' --extract ./LDpruning/train_5E6_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_train_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_train_extract_'+hsq+' --extract ./LDpruning/train_5E5_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_train_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_train_extract_'+hsq+' --extract ./LDpruning/train_5E4_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_train_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_train_extract_'+hsq+' --extract ./LDpruning/train_5E3_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_train_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_train_5E6_SNP --recodeA --out ./final/'+index+'_'+hsq+'_train_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_train_5E5_SNP --recodeA --out ./final/'+index+'_'+hsq+'_train_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_train_5E4_SNP --recodeA --out ./final/'+index+'_'+hsq+'_train_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_train_5E3_SNP --recodeA --out ./final/'+index+'_'+hsq+'_train_5E3_recodeA')\n",
    "\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_valid_extract_'+hsq+' --extract ./LDpruning/train_5E6_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_valid_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_valid_extract_'+hsq+' --extract ./LDpruning/train_5E5_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_valid_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_valid_extract_'+hsq+' --extract ./LDpruning/train_5E4_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_valid_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_valid_extract_'+hsq+' --extract ./LDpruning/train_5E3_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_valid_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_valid_5E6_SNP --recodeA --out ./final/'+index+'_'+hsq+'_valid_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_valid_5E5_SNP --recodeA --out ./final/'+index+'_'+hsq+'_valid_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_valid_5E4_SNP --recodeA --out ./final/'+index+'_'+hsq+'_valid_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_valid_5E3_SNP --recodeA --out ./final/'+index+'_'+hsq+'_valid_5E3_recodeA')\n",
    "\n",
    "\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E6_test_extract_'+hsq+' --extract ./LDpruning/train_5E6_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_test_5E6_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E5_test_extract_'+hsq+' --extract ./LDpruning/train_5E5_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_test_5E5_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E4_test_extract_'+hsq+' --extract ./LDpruning/train_5E4_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_test_5E4_SNP')\n",
    "    os.system('plink --bfile ./SNPextraction/'+index+'_5E3_test_extract_'+hsq+' --extract ./LDpruning/train_5E3_'+index+'_'+hsq+'.prune.in --make-bed --out ./final/'+index+'_'+hsq+'_test_5E3_SNP')\n",
    "\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_test_5E6_SNP --recodeA --out ./final/'+index+'_'+hsq+'_test_5E6_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_test_5E5_SNP --recodeA --out ./final/'+index+'_'+hsq+'_test_5E5_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_test_5E4_SNP --recodeA --out ./final/'+index+'_'+hsq+'_test_5E4_recodeA')\n",
    "    os.system('plink --bfile ./final/'+index+'_'+hsq+'_test_5E3_SNP --recodeA --out ./final/'+index+'_'+hsq+'_test_5E3_recodeA')\n",
    "\n",
    "\n",
    "for hsq in [0.5]:\n",
    "    for index in range(1,11):\n",
    "        generate(str(index),str(hsq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import multiprocessing\n",
    "import operator\n",
    "import scipy\n",
    "from itertools import groupby\n",
    "from operator import itemgetter \n",
    "def generate_G_P(path_data):\n",
    "    data=np.genfromtxt(path_data,dtype=str)\n",
    "    SNPindexlist=[]\n",
    "    SNP_id=[]\n",
    "    all_id=[]\n",
    "    #indindexlist=[]\n",
    "    ind_genotype=[]\n",
    "    all_disease=[]\n",
    "    totalind=data.shape[0]\n",
    "    #print(totalind)\n",
    "    for i in range(totalind):\n",
    "        #print(i)\n",
    "        if i==0:\n",
    "            snpnum=data[0].shape[0]\n",
    "            for j in range(6,snpnum):\n",
    "                SNPindexlist.append(j)\n",
    "                SNP_id.append(data[0][j].split('_')[0])\n",
    "        else:\n",
    "            if len(SNPindexlist)==1:\n",
    "                tmp=data[i][6]\n",
    "                if tmp=='NA':\n",
    "                    tmp='0'\n",
    "            else:\n",
    "                tmp=list(itemgetter(*SNPindexlist)(data[i]))\n",
    "                for x in range(len(tmp)):\n",
    "                    if tmp[x]=='NA':\n",
    "                        tmp[x]='0'\n",
    "            #indindexlist.append(indall.index(data[i][0]))\n",
    "            ind_genotype.append(list(map(int,tmp)))\n",
    "            all_id.append(data[i][0])\n",
    "            all_disease.append(int(data[i][5])-1)\n",
    "    all_genotype=sta.zscore(ind_genotype)\n",
    "    #print(all_genotype[0:5])\n",
    "    return all_genotype,all_disease,all_id,SNP_id\n",
    "\n",
    "\n",
    "def Cal_score(index,hsq,pv):\n",
    "    #print(index)\n",
    "    outpath1='./train/'\n",
    "    outpath2='./valid/'\n",
    "    outpath3='./test/'\n",
    "\n",
    "    out1_geno=outpath1+str(index)+'_'+hsq+'_5E{}_genotype_train'.format(pv)\n",
    "    out2_geno=outpath2+str(index)+'_'+hsq+'_5E{}_genotype_valid'.format(pv)\n",
    "    out3_geno=outpath3+str(index)+'_'+hsq+'_5E{}_genotype_test'.format(pv)\n",
    "\n",
    "\n",
    "    out1_disease=outpath1+str(index)+'_'+hsq+'_disease_train'\n",
    "    out2_disease=outpath2+str(index)+'_'+hsq+'_disease_valid'\n",
    "    out3_disease=outpath3+str(index)+'_'+hsq+'_disease_test'\n",
    "    \n",
    "    out1_id=outpath1+str(index)+'_'+hsq+'_indid_train'\n",
    "    out2_id=outpath2+str(index)+'_'+hsq+'_indid_valid'\n",
    "    out3_id=outpath3+str(index)+'_'+hsq+'_indid_test'\n",
    "    \n",
    "    out1_SNP = outpath1+str(index)+'_'+hsq+'_5E{}_SNP_train'.format(pv)\n",
    "    out2_SNP = outpath2+str(index)+'_'+hsq+'_5E{}_SNP_valid'.format(pv)\n",
    "    out3_SNP = outpath3+str(index)+'_'+hsq+'_5E{}_SNP_test'.format(pv)\n",
    "        \n",
    "    [all_genotype_1,all_disease_1,all_id_1,SNP_id_1]=generate_G_P(dir_path+index+'_'+hsq+'_train_5E{}_recodeA.raw'.format(pv))\n",
    "    [all_genotype_2,all_disease_2,all_id_2,SNP_id_2]=generate_G_P(dir_path+index+'_'+hsq+'_valid_5E{}_recodeA.raw'.format(pv))\n",
    "    [all_genotype_3,all_disease_3,all_id_3,SNP_id_3]=generate_G_P(dir_path+index+'_'+hsq+'_test_5E{}_recodeA.raw'.format(pv))\n",
    "  \n",
    "\n",
    "    np.save(out1_geno, all_genotype_1)\n",
    "    np.save(out2_geno, all_genotype_2)\n",
    "    np.save(out3_geno, all_genotype_3)\n",
    "\n",
    "\n",
    "    np.save(out1_disease,all_disease_1)    \n",
    "    np.save(out2_disease,all_disease_2)\n",
    "    np.save(out3_disease,all_disease_3)\n",
    "\n",
    "    np.save(out1_id,all_id_1)\n",
    "    np.save(out2_id,all_id_2)\n",
    "    np.save(out3_id,all_id_3)\n",
    "\n",
    "    np.save(out1_SNP,SNP_id_1)\n",
    "    np.save(out2_SNP,SNP_id_2)\n",
    "    np.save(out3_SNP,SNP_id_3)\n",
    "\n",
    "#indall=[]\n",
    "#fam_path = os.path.abspath('..')\n",
    "#infile1=open('{}/sim_chr1.fam'.format(fam_path))\n",
    "#for line in infile1:\n",
    "#    A=line.strip('\\n').split()\n",
    "#    indall.append(A[0])\n",
    "#infile1.close()\n",
    "\n",
    "\n",
    "dir_path='./final/'\n",
    "pool = multiprocessing.Pool(10)\n",
    "for index in range(1,11):\n",
    "    for hsq in [0.5]:\n",
    "        for pv in [3,4,5,6]:\n",
    "            pool.apply_async(Cal_score,(str(index),str(hsq),pv,))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "\n",
    "def predict(index):\n",
    "    global auc\n",
    "    global name\n",
    "    global ap\n",
    "    #print(index)\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    #disease_valid=np.load(dir_path2+index+'_'+hsq+'_disease_valid.npy')\n",
    "    #disease_test=np.load(dir_path3+index+'_'+hsq+'_disease_test.npy')\n",
    "\n",
    "    Cxlist=[DecisionTreeClassifier(),LogisticRegression(),ExtraTreeClassifier(),GaussianNB()]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        #print(s)\n",
    "        valid=0\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                ada = AdaBoostClassifier(base_estimator=Cx)\n",
    "                ada.fit(Geno_train,disease_train)\n",
    "                Y=ada.predict_proba(Geno_valid)\n",
    "                np.save('./ada/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    max_index=set1.index(max(set1))\n",
    "    s1=int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    ada1 = AdaBoostClassifier(base_estimator=Cx1)\n",
    "    ada1.fit(Geno_train,disease_train)\n",
    "    Y1=ada1.predict_proba(Geno_test)\n",
    "    np.save('./ada/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./ada/test_predi_score_{}_ada.txt'.format(index),sep=' ',index=False,header=False)\n",
    " \n",
    "name=[]\n",
    "auc=[]\n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(4)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_ada.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_ada.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import normfrom collections import defaultdict\n",
    "import matplotlib\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "\n",
    "def predict(index):\n",
    "    #print(index)\n",
    "    global name\n",
    "    global auc\n",
    "    global ap\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    Cxlist=[0.0001,0.001,0.01,0.1]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        valid=0\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                RF = RandomForestClassifier(min_impurity_decrease=Cx)\n",
    "                RF.fit(Geno_train,disease_train)\n",
    "                Y=RF.predict_proba(Geno_valid)\n",
    "                #print(Y)\n",
    "                np.save('./RF/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    max_index=set1.index(max(set1))\n",
    "    s1=int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    RF1=RandomForestClassifier(min_impurity_decrease=Cx1)\n",
    "    RF1.fit(Geno_train,disease_train)\n",
    "    Y1=RF1.predict_proba(Geno_test)\n",
    "    #print(Y1)\n",
    "    np.save('./RF/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./RF/test_predi_score_{}_RF.txt'.format(index),sep=' ',index=False,header=False)\n",
    "\n",
    "name=[]\n",
    "auc=[] \n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(4)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_RF.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_RF.txt',sep='\\t',header=False,index=False)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "\n",
    "\n",
    "def predict(index):\n",
    "    #print(index)\n",
    "    global auc\n",
    "    global name\n",
    "    global ap\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    Cxlist=[0.0001,0.001,0.01,0.1]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        #print(s)\n",
    "        valid=0\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                GB = GradientBoostingClassifier(min_impurity_decrease=Cx)\n",
    "                GB.fit(Geno_train,disease_train)\n",
    "                Y=GB.predict_proba(Geno_valid)\n",
    "                np.save('./GB/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    max_index=set1.index(max(set1))\n",
    "    s1=int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    #Geno_valid=np.load(dir_path2+index+'_'+hsq+'_5E{}_genotype_valid.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    GB1 =GradientBoostingClassifier(min_impurity_decrease=Cx1)\n",
    "    GB1.fit(Geno_train,disease_train)\n",
    "    Y1=GB1.predict_proba(Geno_test)\n",
    "    np.save('./GB/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./GB/test_predi_score_{}_GB.txt'.format(index),sep=' ',index=False,header=False)\n",
    "    #print('index:'+str(index))\n",
    "    #print('set1:'+str(len(set1)))\n",
    "    #print('auc:'+str(len(auc)))\n",
    "    #print('name:'+str(len(name)))\n",
    "    #print('done')\n",
    "name=[]\n",
    "auc=[]\n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(4)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_GB.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_GB.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from  sklearn.neural_network import MLPClassifier\n",
    "\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "def predict(index):\n",
    "    #print(index)\n",
    "    global auc\n",
    "    global name\n",
    "    global ap\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    Cxlist=[0.0001,0.001,0.01,0.1]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        #print(s)\n",
    "        valid=0\n",
    "        print(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                LR = LogisticRegression(penalty='l1', C=Cx,solver='liblinear', max_iter=10000)\n",
    "                LR.fit(Geno_train,disease_train)\n",
    "                Y=LR.predict_proba(Geno_valid)\n",
    "                np.save('./LR/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    print(len(set1))\n",
    "    max_index=set1.index(max(set1))\n",
    "    s1 = int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    LR1 = LogisticRegression(penalty='l1', C=Cx1,solver='liblinear', max_iter=10000)\n",
    "    LR1.fit(Geno_train,disease_train)\n",
    "    Y1=LR1.predict_proba(Geno_test)\n",
    "    np.save('./LR/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./LR/test_predi_score_{}_LR.txt'.format(index),sep=' ',index=False,header=False)\n",
    "\n",
    "name=[]\n",
    "auc=[]\n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(4)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_LR.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_LR.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from  sklearn.neural_network import MLPClassifier\n",
    "\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "\n",
    "\n",
    "def predict(index):\n",
    "    #print(index)\n",
    "    global name\n",
    "    global auc\n",
    "    global ap\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    Cxlist=[(30,30),(10,10,10,10),(20,20,20,20),(5,5,5,5)]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        #print(s)\n",
    "        valid=0\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                NN=MLPClassifier(hidden_layer_sizes=Cx,max_iter=1000)\n",
    "                NN.fit(Geno_train,disease_train)\n",
    "                Y=NN.predict_proba(Geno_valid)\n",
    "                np.save('./NN/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    max_index=set1.index(max(set1))\n",
    "    #print('len(set1):'+str(len(set1)))\n",
    "    s1=int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    #print('name:'+str(len(name)))\n",
    "    NN1=MLPClassifier(hidden_layer_sizes=Cx1,max_iter=1000)\n",
    "    NN1.fit(Geno_train,disease_train)\n",
    "    Y1=NN1.predict_proba(Geno_test)\n",
    "    np.save('./NN/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    #print('len(set5):'+str(len(set5)))\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./NN/test_predi_score_{}_NN.txt'.format(index),sep=' ',index=False,header=False)\n",
    "\n",
    "name=[]\n",
    "auc=[]\n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(5)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_NN.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_NN.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "from scipy import stats as sta\n",
    "import math\n",
    "import pandas as pd\n",
    "import operator\n",
    "from itertools import groupby\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from operator import itemgetter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import multiprocessing\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dir_path1='./train/'\n",
    "dir_path2='./valid/'\n",
    "dir_path3='./test/'\n",
    "\n",
    "def predict(index):\n",
    "    #print(index)\n",
    "    global name\n",
    "    global auc\n",
    "    global ap\n",
    "    disease_train=np.load(dir_path1+index+'_disease_train.npy')\n",
    "    Cxlist=[0.0001,0.001,0.01,0.1]\n",
    "    set1=[]\n",
    "    for s in [1,2,3,4]:\n",
    "        print(s)\n",
    "        valid=0\n",
    "        if s==1 and os.path.exists(dir_path1+index+'_5E3_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E3_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E3_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E3_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==2 and os.path.exists(dir_path1+index+'_5E4_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E4_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E4_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E4_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==3 and os.path.exists(dir_path1+index+'_5E5_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E5_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E5_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E5_genotype_test.npy')\n",
    "            valid=1\n",
    "        elif s==4 and os.path.exists(dir_path1+index+'_5E6_genotype_train.npy'):\n",
    "            Geno_train=np.load(dir_path1+index+'_5E6_genotype_train.npy')\n",
    "            Geno_valid=np.load(dir_path2+index+'_5E6_genotype_valid.npy')\n",
    "            Geno_test=np.load(dir_path3+index+'_5E6_genotype_test.npy')\n",
    "            valid=1\n",
    "        if valid==1:\n",
    "            for t in [1,2,3,4]:\n",
    "                Cx=Cxlist[t-1]\n",
    "                RF = RandomForestClassifier(min_impurity_decrease=Cx)\n",
    "                RF.fit(Geno_train,disease_train)\n",
    "                Y=RF.predict_proba(Geno_valid)\n",
    "                #print(Y)\n",
    "                np.save('./RF/'+index+'_'+str(s)+'_'+str(t)+'_valid.npy',Y)\n",
    "                disease_valid=pd.read_csv('../{}/{}_valid_new.fam'.format(index,index),sep=' +',header=None)\n",
    "                list1=[]\n",
    "                for i in range(len(disease_valid.index)):\n",
    "                    if disease_valid.iloc[i,5]==-9:\n",
    "                        list1.append(i)\n",
    "                disease_valid.drop(list1,axis=0,inplace=True)\n",
    "                disease_valid.index=list(range(len(disease_valid.index)))\n",
    "                set2=[]\n",
    "                for j in range(len(disease_valid.index)):\n",
    "                    set2.append(Y[j][1])\n",
    "                set3 = [int(x)-1 for x in disease_valid.iloc[:,5].values]\n",
    "                set1.append(roc_auc_score(set3,set2))\n",
    "    max_index=set1.index(max(set1))\n",
    "    s1=int((max_index)/4)+1\n",
    "    Cx1=Cxlist[max_index-(s1-1)*4]\n",
    "    name.append('{}_s{}_t{}'.format(index,s1,Cx1))\n",
    "    Geno_train=np.load(dir_path1+index+'_5E{}_genotype_train.npy'.format(s1+2))\n",
    "    Geno_test=np.load(dir_path3+index+'_5E{}_genotype_test.npy'.format(s1+2))\n",
    "    RF1=RandomForestClassifier(min_impurity_decrease=Cx1)\n",
    "    RF1.fit(Geno_train,disease_train)\n",
    "    Y1=RF1.predict_proba(Geno_test)\n",
    "    #print(Y1)\n",
    "    np.save('./RF/'+index+'_'+str(s)+'_'+str(t)+'_test.npy',Y1)\n",
    "    disease_test=pd.read_csv('../{}/{}_test_new.fam'.format(index,index),sep=' +',header=None)\n",
    "    list2=[]\n",
    "    for k1 in range(len(disease_test.index)):\n",
    "        if disease_test.iloc[k1,5]==-9:\n",
    "            list2.append(k1)\n",
    "    disease_test.drop(list2,axis=0,inplace=True)\n",
    "    disease_test.index=list(range(len(disease_test.index)))\n",
    "    set4=[]\n",
    "    for k2 in range(len(disease_test.index)):\n",
    "        set4.append(Y1[k2][1])\n",
    "    set5 = [int(x)-1 for x in disease_test.iloc[:,5].values]\n",
    "    auc.append(roc_auc_score(set5,set4))\n",
    "    ap.append(average_precision_score(set5,set4))\n",
    "    df_score = pd.DataFrame(disease_test.iloc[:,0].values)\n",
    "    df_score[1] = set4\n",
    "    df_score[2] = set5\n",
    "    df_score.to_csv('./RF/test_predi_score_{}_RF.txt'.format(index),sep=' ',index=False,header=False)\n",
    "\n",
    "name=[]\n",
    "auc=[] \n",
    "ap=[]\n",
    "#pool = multiprocessing.Pool(4)\n",
    "\n",
    "diseases=['1002','1348','1066','1111','1134','1154','1220','1242','1265','1294','1297','1374','1065','1068','1113','1136','1207','1224','1243','1293','1295','1452']\n",
    "for index in diseases:\n",
    "    #print(m)\n",
    "    predict(index)\n",
    "    #pool.apply_async(predict,(index,))\n",
    "#pool.close()\n",
    "#pool.join()\n",
    "\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('test_auc_results_RF.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('test_ap_results_RF.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the script below, 'new_best_vali_auc_{}.txt' is a file consists of one row and two columns. A row stands for a disease, and the first column is a string consists of the disease name and the best tuning parameters of methods ('best' means highest AUROC), they are linked by '_' (e.g. diabetes_1_0.5, diabetes is the disease name, 1 and 0.5 are the two tuning parameters for a method), and the second column is AUROC calculated by a method for a disease. '.profile' file is the output file of PLINK when calculating PRS. 'new_score*.profile' is the PRS calculated on the validation set. 'new_test_score*.profile' is the PRS calculated on the test set. The suffix of files containing PRS of PRSice and PRSice2 is '.best' but not '.profile'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "methods = ['PRScs','LDpred2','P+T','PRSice','DBSLMM','Tweedie','SBLUP','SbayesR','PRSice2','lassosum','EBPRS','tlpSum','elastSum']\n",
    "\n",
    "for me in methods:\n",
    "    if me == 'tlpSum' or me == 'elastSum':\n",
    "        path = './penRegSum/'\n",
    "        exec(\"df_{} = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(me,path,me))\n",
    "        exec('auc_{} = df_{}[1].to_list()'.format(me,me))\n",
    "    elif me == 'P+T':\n",
    "        path = './{}/'.format(me)\n",
    "        exec(\"df_PandT = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(path,me))\n",
    "        exec('auc_PandT = df_PandT[1].to_list()')\n",
    "    else:\n",
    "        path = './{}/'.format(me)\n",
    "        exec(\"df_{} = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(me,path,me))\n",
    "        exec('auc_{} = df_{}[1].to_list()'.format(me,me))\n",
    "\n",
    "auc = []\n",
    "ap = []\n",
    "name = []\n",
    "row_length = 1 #The length of row is 1 here as we have 1 disease in one 'new_best_vali_auc*.txt' file.\n",
    "for i in range(row_length):\n",
    "    for me in methods:\n",
    "        if me == 'PRScs':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_PRScs_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_PRScs.index)):\n",
    "                if df1_PRScs.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_PRScs.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_PRScs_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRScs.index)):\n",
    "                if df2_PRScs.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRScs.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "        elif me == 'LDpred2':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_LDpred2_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_LDpred2.index)):\n",
    "                if df1_LDpred2.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_LDpred2.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_LDpred2_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_LDpred2.index)):\n",
    "                if df2_LDpred2.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_LDpred2.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "        elif me == 'DBSLMM':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_DBSLMM_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_DBSLMM.index)):\n",
    "                if df1_DBSLMM.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_DBSLMM.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_DBSLMM_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_DBSLMM.index)):\n",
    "                if df2_DBSLMM.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_DBSLMM.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "        elif me == 'SBLUP':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_SBLUP_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_SBLUP.index)):\n",
    "                if df1_SBLUP.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_SBLUP.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_SBLUP_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_SBLUP.index)):\n",
    "                if df2_SBLUP.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_SBLUP.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "        elif me == 'SbayesR':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_SbayesR_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_SbayesR.index)):\n",
    "                if df1_SbayesR.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_SbayesR.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_SbayesR_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_SbayesR.index)):\n",
    "                if df2_SbayesR.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_SbayesR.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "        elif me == 'P+T':\n",
    "            exec(\"para_set = df_PandT.iloc[i,0].split('_')\")\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_PandT = pd.read_csv('./{}/new_score_P+T_{}_pv{}_r{}.profile',sep=' +')\".format(me,rep,para1,para2))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_PandT.index)):\n",
    "                if df1_PandT.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_PandT.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_PandT = df1_PandT.loc[:,'SCORE'].to_list()\")\n",
    "            exec(\"df2_PandT = pd.read_csv('./{}/new_test_score_P+T_{}_{}_{}.profile',sep=' +')\".format(me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PandT.index)):\n",
    "                if df2_PandT.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PandT.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_PandT = df2_PandT.loc[:,'SCORE'].to_list()\")\n",
    "        elif me == 'PRSice':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_PRSice_{}_{}_{}.best',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            fam_PRSice = pd.read_csv('./{}_valid_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df1_PRSice['PHENO'] = fam_PRSice.iloc[:,5].values\n",
    "            set1 = []\n",
    "            for j in range(len(df1_PRSice.index)):\n",
    "                if df1_PRSice.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_PRSice.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'PRS'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_PRSice_{}_{}_{}.best',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            test_fam_PRSice = pd.read_csv('./{}_test_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df2_PRSice['PHENO'] = test_fam_PRSice.iloc[:,5].values\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRSice.index)):\n",
    "                if df2_PRSice.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRSice.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'PRS'].to_list()\".format(me,me))\n",
    "        elif me == 'PRSice2':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_PRSice2_{}.best',sep=' +')\".format(me,me,rep))\n",
    "            fam_PRSice2 = pd.read_csv('./{}_valid_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df1_PRSice2['PHENO'] = fam_PRSice2.iloc[:,5].values\n",
    "            set1 = []\n",
    "            for j in range(len(df1_PRSice2.index)):\n",
    "                if df1_PRSice2.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_PRSice2.drop(set1,axis=0,inplace=True)\n",
    "            exec(\"score_{} = df1_{}.loc[:,'PRS'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_PRSice2_{}.best',sep=' +')\".format(me,me,rep))\n",
    "            test_fam_PRSice2 = pd.read_csv('./{}_test_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df2_PRSice2['PHENO'] = test_fam_PRSice2.iloc[:,5].values\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRSice2.index)):\n",
    "                if df2_PRSice2.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRSice2.drop(set2,axis=0,inplace=True)\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'PRS'].to_list()\".format(me,me))\n",
    "        elif me == 'Tweedie':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_Tweedie.index)):\n",
    "                if df1_Tweedie.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_Tweedie.drop(set1,axis=0,inplace=True)\n",
    "            valid_pheno = df1_Tweedie.loc[:,'PHENO'].to_list()\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_Tweedie.index)):\n",
    "                if df2_Tweedie.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_Tweedie.drop(set2,axis=0,inplace=True)\n",
    "            test_pheno = df2_Tweedie.loc[:,'PHENO'].to_list()\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "        elif me == 'lassosum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_lassosum.index)):\n",
    "                if df1_lassosum.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_lassosum.drop(set1,axis=0,inplace=True)\n",
    "            valid_pheno = df1_lassosum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_lassosum.index)):\n",
    "                if df2_lassosum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_lassosum.drop(set2,axis=0,inplace=True)\n",
    "            test_pheno = df2_lassosum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "        elif me == 'EBPRS':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df1_{} = pd.read_csv('./{}/new_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_EBPRS.index)):\n",
    "                if df1_EBPRS.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_EBPRS.drop(set1,axis=0,inplace=True)\n",
    "            valid_pheno = df1_EBPRS.loc[:,'PHENO'].to_list()\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_EBPRS.index)):\n",
    "                if df2_EBPRS.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_EBPRS.drop(set2,axis=0,inplace=True)\n",
    "            test_pheno = df2_EBPRS.loc[:,'PHENO'].to_list()\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "        elif me == 'tlpSum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_{} = pd.read_csv('./penRegSum/new_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_tlpSum.index)):\n",
    "                if df1_tlpSum.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_tlpSum.drop(set1,axis=0,inplace=True)\n",
    "            valid_pheno = df1_tlpSum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./penRegSum/new_test_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_tlpSum.index)):\n",
    "                if df2_tlpSum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_tlpSum.drop(set2,axis=0,inplace=True)\n",
    "            test_pheno = df2_tlpSum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "        elif me == 'elastSum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df1_{} = pd.read_csv('./penRegSum/new_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set1 = []\n",
    "            for j in range(len(df1_elastSum.index)):\n",
    "                if df1_elastSum.loc[j,'PHENO'] == -9:\n",
    "                    set1.append(j)\n",
    "            df1_elastSum.drop(set1,axis=0,inplace=True)\n",
    "            valid_pheno = df1_elastSum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"score_{} = df1_{}.loc[:,'SCORE'].to_list()\".format(me,me))\n",
    "            exec(\"df2_{} = pd.read_csv('./penRegSum/new_test_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_elastSum.index)):\n",
    "                if df2_elastSum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_elastSum.drop(set2,axis=0,inplace=True)\n",
    "            test_pheno = df2_elastSum.loc[:,'PHENO'].to_list()\n",
    "            exec(\"test_score_{} = df2_{}.loc[:,'SCORE'].values\".format(me,me))\n",
    "    df_valid_score = np.array([score_PRScs,score_LDpred2,score_PandT,score_PRSice,score_DBSLMM,score_Tweedie,score_SBLUP,score_SbayesR,score_lassosum,score_EBPRS,score_tlpSum,score_elastSum,score_PRSice2])\n",
    "    if df_valid_score.shape[1] != 13:\n",
    "        df_valid_score = df_valid_score.T\n",
    "    LR = LogisticRegression(max_iter=5000)\n",
    "    valid_pheno = [int(x-1) for x in valid_pheno]\n",
    "    LR.fit(df_valid_score,valid_pheno)\n",
    "    df_test_score = np.array([test_score_PRScs,test_score_LDpred2,test_score_PandT,test_score_PRSice,test_score_DBSLMM,test_score_Tweedie,test_score_SBLUP,test_score_SbayesR,test_score_lassosum,test_score_EBPRS,test_score_tlpSum,test_score_elastSum,test_score_PRSice2])\n",
    "    if df_test_score.shape[1] != 13:\n",
    "        df_test_score = df_test_score.T\n",
    "    Y = LR.predict_proba(df_test_score)\n",
    "    test_pheno = [int(x-1) for x in test_pheno]\n",
    "    auc.append(roc_auc_score(test_pheno,Y[:,1]))\n",
    "    ap.append(average_precision_score(test_pheno,Y[:,1]))\n",
    "    name.append('{}'.format(rep))\n",
    "    df_score = pd.DataFrame(df2_elastSum.iloc[:,0].values)\n",
    "    df_score[1] = Y[:,1]\n",
    "    df_score[2] = test_pheno\n",
    "    df_score.to_csv('new_test_predi_score_ensem.txt',sep='\\t',header=False,index=False)\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df.to_csv('new_test_auc_ensem.txt',sep='\\t',header=False,index=False)\n",
    "df1 = pd.DataFrame(name)\n",
    "df1[1] = ap\n",
    "df1.to_csv('new_test_ap_ensem.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRS calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('plink --bfile prefix_of_bed_file --score SNP_effect_sizes_file variant_IDs_column allele_codes_column SNP_effect_sizes_column --out output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUROC and AUPRC evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "name = []\n",
    "auc = []\n",
    "ap = []\n",
    "pro = pd.read_csv('.profile file from PLINK',sep=' +')\n",
    "set1 = []\n",
    "for j in range(len(pro.index)):\n",
    "    if pro.loc[j,'PHENO']==-9:\n",
    "        set1.append(j)\n",
    "pro.drop(set1,axis=0,inplace=True) #remove individuals of phenotypes being -9\n",
    "y_true1 = pro.loc[:,'PHENO'].values\n",
    "y_true = [abs(int(x)-1) for x in y_true1] #change disease label from 1 and 2 to 0 and 1\n",
    "y_score = pro.loc[:,'SCORE'].values\n",
    "name.append() #disease name and tuning parameters\n",
    "auc.append(roc_auc_score(y_true,y_score))\n",
    "ap.append(average_precision_score(y_true,y_score))\n",
    "df = pd.DataFrame(name)\n",
    "df[1] = auc\n",
    "df[2] = ap\n",
    "df.to_csv('results.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odds ratio evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "methods = ['PRScs','LDpred2','P+T','PRSice','DBSLMM','Tweedie','SBLUP','SbayesR','PRSice2','lassosum','EBPRS','tlpSum','elastSum','ensem']\n",
    "ML_methods = ['ada','GB','LR','NN','RF']\n",
    "\n",
    "for me in methods:\n",
    "    if me == 'tlpSum' or me == 'elastSum':\n",
    "        path = './penRegSum/'\n",
    "        exec(\"df_{} = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(me,path,me))\n",
    "        exec('auc_{} = df_{}[1].to_list()'.format(me,me))\n",
    "    elif me == 'P+T':\n",
    "        path = './{}/'.format(me)\n",
    "        exec(\"df_PandT = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(path,me))\n",
    "        exec('auc_PandT = df_PandT[1].to_list()')\n",
    "    elif me == 'ensem':\n",
    "        pass\n",
    "    else:\n",
    "        path = './{}/'.format(me)\n",
    "        exec(\"df_{} = pd.read_csv('{}new_best_vali_auc_{}.txt',sep='\\t',header=None,engine='python')\".format(me,path,me))\n",
    "        exec('auc_{} = df_{}[1].to_list()'.format(me,me))\n",
    "\n",
    "\n",
    "row_length = 1 #The length of row is 1 here as we have 1 disease in one 'new_best_vali_auc*.txt' file.\n",
    "ratio = 0.01 #top ratio% versus the rest (1-ratio)%\n",
    "for i in range(row_length):\n",
    "    OR_set = []\n",
    "    p_set = []\n",
    "    confi_set = []\n",
    "    name_set = []\n",
    "    for me in methods:\n",
    "        if me == 'PRScs':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_PRScs_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRScs.index)):\n",
    "                if df2_PRScs.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRScs.drop(set2,axis=0,inplace=True)\n",
    "            df2_PRScs.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_PRScs.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_PRScs.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_PRScs.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_PRScs.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_PRScs.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_PRScs.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'LDpred2':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_LDpred2_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_LDpred2.index)):\n",
    "                if df2_LDpred2.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_LDpred2.drop(set2,axis=0,inplace=True)\n",
    "            df2_LDpred2.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_LDpred2.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_LDpred2.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_LDpred2.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_LDpred2.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_LDpred2.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_LDpred2.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'DBSLMM':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_DBSLMM_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_DBSLMM.index)):\n",
    "                if df2_DBSLMM.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_DBSLMM.drop(set2,axis=0,inplace=True)\n",
    "            df2_DBSLMM.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_DBSLMM.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_DBSLMM.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_DBSLMM.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_DBSLMM.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_DBSLMM.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_DBSLMM.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'SBLUP':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_SBLUP_{}_{}.profile',sep=' +')\".format(me,me,rep,para1))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_SBLUP.index)):\n",
    "                if df2_SBLUP.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_SBLUP.drop(set2,axis=0,inplace=True)\n",
    "            df2_SBLUP.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_SBLUP.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_SBLUP.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_SBLUP.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_SBLUP.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_SBLUP.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_SBLUP.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'SbayesR':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_SbayesR_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_SbayesR.index)):\n",
    "                if df2_SbayesR.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_SbayesR.drop(set2,axis=0,inplace=True)\n",
    "            df2_SbayesR.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_SbayesR.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_SbayesR.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_SbayesR.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_SbayesR.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_SbayesR.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_SbayesR.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'P+T':\n",
    "            exec(\"para_set = df_PandT.iloc[i,0].split('_')\")\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_PandT = pd.read_csv('./{}/new_test_score_P+T_{}_{}_{}.profile',sep=' +')\".format(me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PandT.index)):\n",
    "                if df2_PandT.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PandT.drop(set2,axis=0,inplace=True)\n",
    "            df2_PandT.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_PandT.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_PandT.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_PandT.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_PandT.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_PandT.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_PandT.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'PRSice':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_PRSice_{}_{}_{}.best',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            test_fam_PRSice = pd.read_csv('./{}_test_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df2_PRSice['PHENO'] = test_fam_PRSice.iloc[:,5].values\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRSice.index)):\n",
    "                if df2_PRSice.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRSice.drop(set2,axis=0,inplace=True)\n",
    "            df2_PRSice.drop(['FID','IID','In_Regression'],axis=1,inplace=True)\n",
    "            df2_PRSice.sort_values(by='PRS',ascending=False,inplace=True)\n",
    "            row_num = len(df2_PRSice.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_PRSice.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_PRSice.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_PRSice.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_PRSice.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'PRSice2':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_PRSice2_{}.best',sep=' +')\".format(me,me,rep))\n",
    "            test_fam_PRSice2 = pd.read_csv('./{}_test_new.fam'.format(rep),sep=' +',header=None)\n",
    "            df2_PRSice2['PHENO'] = test_fam_PRSice2.iloc[:,5].values\n",
    "            set2 = []\n",
    "            for k in range(len(df2_PRSice2.index)):\n",
    "                if df2_PRSice2.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_PRSice2.drop(set2,axis=0,inplace=True)\n",
    "            df2_PRSice2.drop(['FID','IID','In_Regression'],axis=1,inplace=True)\n",
    "            df2_PRSice2.sort_values(by='PRS',ascending=False,inplace=True)\n",
    "            row_num = len(df2_PRSice2.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_PRSice2.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_PRSice2.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_PRSice2.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_PRSice2.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'Tweedie':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_Tweedie.index)):\n",
    "                if df2_Tweedie.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_Tweedie.drop(set2,axis=0,inplace=True)\n",
    "            df2_Tweedie.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_Tweedie.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_Tweedie.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_Tweedie.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_Tweedie.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_Tweedie.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_Tweedie.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'lassosum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_lassosum.index)):\n",
    "                if df2_lassosum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_lassosum.drop(set2,axis=0,inplace=True)\n",
    "            df2_lassosum.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_lassosum.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_lassosum.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_lassosum.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_lassosum.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_lassosum.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_lassosum.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'EBPRS':\n",
    "            exec(\"para_set = df_{}.iloc[i,0]\".format(me))\n",
    "            rep = para_set\n",
    "            exec(\"df2_{} = pd.read_csv('./{}/new_test_score_{}_{}.profile',sep=' +')\".format(me,me,me,rep))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_EBPRS.index)):\n",
    "                if df2_EBPRS.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_EBPRS.drop(set2,axis=0,inplace=True)\n",
    "            df2_EBPRS.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_EBPRS.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_EBPRS.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_EBPRS.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_EBPRS.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_EBPRS.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_EBPRS.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'tlpSum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_{} = pd.read_csv('./penRegSum/new_test_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_tlpSum.index)):\n",
    "                if df2_tlpSum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_tlpSum.drop(set2,axis=0,inplace=True)\n",
    "            df2_tlpSum.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_tlpSum.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_tlpSum.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_tlpSum.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_tlpSum.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_tlpSum.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_tlpSum.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'elastSum':\n",
    "            exec(\"para_set = df_{}.iloc[i,0].split('_')\".format(me))\n",
    "            rep = para_set[0]\n",
    "            para1 = para_set[1]\n",
    "            para2 = para_set[2]\n",
    "            exec(\"df2_{} = pd.read_csv('./penRegSum/new_test_score_{}_{}_{}_{}.profile',sep=' +')\".format(me,me,rep,para1,para2))\n",
    "            set2 = []\n",
    "            for k in range(len(df2_elastSum.index)):\n",
    "                if df2_elastSum.loc[k,'PHENO'] == -9:\n",
    "                    set2.append(k)\n",
    "            df2_elastSum.drop(set2,axis=0,inplace=True)\n",
    "            df2_elastSum.drop(['FID','IID','CNT','CNT2'],axis=1,inplace=True)\n",
    "            df2_elastSum.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_elastSum.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_1 = (df2_elastSum.PHENO[:row_place] == 1).sum()\n",
    "            top_2 = (df2_elastSum.PHENO[:row_place] == 2).sum()\n",
    "            rest_1 = (df2_elastSum.PHENO[row_place:] == 1).sum()\n",
    "            rest_2 = (df2_elastSum.PHENO[row_place:] == 2).sum()\n",
    "            ct = [[top_2,top_1],[rest_2,rest_1]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "        elif me == 'ensem':\n",
    "            df2_ensem = pd.read_csv('new_test_predi_score_ensem.txt',sep='\\t',header=None)\n",
    "            df2_ensem.columns = ['ID','SCORE','PHENO']\n",
    "            df2_ensem.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "            row_num = len(df2_ensem.index)\n",
    "            row_place = int(row_num*ratio)\n",
    "            top_0 = (df2_ensem.PHENO[:row_place] == 0).sum()\n",
    "            top_1 = (df2_ensem.PHENO[:row_place] == 1).sum()\n",
    "            rest_0 = (df2_ensem.PHENO[row_place:] == 0).sum()\n",
    "            rest_1 = (df2_ensem.PHENO[row_place:] == 1).sum()\n",
    "            ct = [[top_1,top_0],[rest_1,rest_0]]\n",
    "            table = sm.stats.Table2x2(ct)\n",
    "            OR = table.oddsratio\n",
    "            confi = table.oddsratio_confint()\n",
    "            p = table.oddsratio_pvalue()\n",
    "            confi_set.append(confi)\n",
    "            p_set.append(p)\n",
    "            name_set.append('{}_{}'.format(rep,me))\n",
    "            OR_set.append(OR)\n",
    "\n",
    "    for ml in ML_methods:\n",
    "        df = pd.read_csv('../ML/{}/test_predi_score_{}_{}.txt'.format(ml,rep,ml),sep=' ',header=None)\n",
    "        df.columns = ['ID','SCORE','PHENO']\n",
    "        df.sort_values(by='SCORE',ascending=False,inplace=True)\n",
    "        row_num = len(df.index)\n",
    "        row_place = int(row_num*ratio)\n",
    "        top_0 = (df.PHENO[:row_place] == 0).sum()\n",
    "        top_1 = (df.PHENO[:row_place] == 1).sum()\n",
    "        rest_0 = (df.PHENO[row_place:] == 0).sum()\n",
    "        rest_1 = (df.PHENO[row_place:] == 1).sum()\n",
    "        ct = [[top_1,top_0],[rest_1,rest_0]]\n",
    "        table = sm.stats.Table2x2(ct)\n",
    "        OR = table.oddsratio\n",
    "        confi = table.oddsratio_confint()\n",
    "        p = table.oddsratio_pvalue()\n",
    "        confi_set.append(confi)\n",
    "        p_set.append(p)\n",
    "        name_set.append('{}_{}'.format(rep,ml))\n",
    "        OR_set.append(OR)\n",
    "    df_tab = pd.DataFrame(name_set)\n",
    "    df_tab[1] = OR_set\n",
    "    df_tab[2] = confi_set\n",
    "    df_tab[3] = p_set\n",
    "    df_tab.to_csv('./new_OR_{}_ratio{}.txt'.format(rep,ratio),sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pearson correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "scores = # the scores of a method for target individuals\n",
    "phenotypes = # the phenotypes of target individuals\n",
    "pearson = stats.pearsonr(scores,phenotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spearman's rank correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "score1 = # the scores of method 1 for target individuals\n",
    "score2 = # the scores of method 2 for target individuals\n",
    "spearman = stats.spearmanr(score1,score1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
